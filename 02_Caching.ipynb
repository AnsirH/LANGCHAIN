{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ea8d20",
   "metadata": {},
   "source": [
    "# LangChain을 활용한 모델 사용, 비용 모니터링 및 캐싱 전략\n",
    "```\n",
    "GPT-4o-mini: GPT-3.5-Turbo보다 비용이 60% 저렴\n",
    "LangChain V0.3x 부터 openAI 가 별도 패키지로 분리 필요 패키지를 설치 langchain_openai 필요\n",
    "토큰 사용량 추적, 캐싱을 위한 langchain-community도 별도 설치\n",
    "환경 변수 관린 패키지 python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "045e7894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement python-doenv (from versions: none)\n",
      "ERROR: No matching distribution found for python-doenv\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-openai langchain-community python-doenv openai -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6001cd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40076451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 자연어 처리(NLP) 모델을 활용하여 다양한 애플리케이션을 구축하고 통합할 수 있도록 지원하는 프레임워크입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "prompt = \"LangChain에 대해 한 문장으로 설명해줘.\"\n",
    "result = llm.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "028e7ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 19,\n",
       " 'output_tokens': 34,\n",
       " 'total_tokens': 53,\n",
       " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       " 'output_token_details': {'audio': 0, 'reasoning': 0}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용량\n",
    "result.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ebef382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-community -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b4793a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응답1: 죄송하지만, 실시간 날씨 정보는 제공할 수 없습니다. 서울의 현재 날씨를 확인하시려면 기상청 웹사이트나 날씨 앱을 참고하시기 바랍니다. 도움이 필요하시면 다른 질문이 있으신가요?\n",
      "응답2: LangChain은 다양한 자연어 처리 작업을 수행할 수 있는 파이썬 라이브러리입니다. 주로 언어 모델과 상호작용하거나, 다양한 데이터 소스와 연결하여 복잡한 애플리케이션을 만드는 데 사용됩니다. LangChain을 사용하기 위해서는 몇 가지 단계를 따라야 합니다.\n",
      "\n",
      "### 1. 환경 설정\n",
      "\n",
      "먼저, Python 환경이 필요합니다. Python 3.7 이상이 설치되어 있어야 하며, `pip`를 사용하여 LangChain을 설치할 수 있습니다.\n",
      "\n",
      "```bash\n",
      "pip install langchain\n",
      "```\n",
      "\n",
      "### 2. 기본 사용법\n",
      "\n",
      "LangChain을 사용하는 기본적인 예제를 보여드리겠습니다. 아래 코드는 LangChain을 사용하여 간단한 언어 모델을 호출하는 예입니다.\n",
      "\n",
      "```python\n",
      "from langchain import LLMChain\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "# OpenAI API 키 설정\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
      "\n",
      "# 언어 모델 인스턴스 생성\n",
      "llm = OpenAI(model=\"text-davinci-003\")\n",
      "\n",
      "# LLMChain 생성\n",
      "chain = LLMChain(llm=llm)\n",
      "\n",
      "# 입력 텍스트\n",
      "prompt = \"Python의 장점은 무엇인가요?\"\n",
      "\n",
      "# 결과 생성\n",
      "result = chain.run(prompt)\n",
      "\n",
      "print(result)\n",
      "```\n",
      "\n",
      "### 3. 구성 요소\n",
      "\n",
      "- **LLM (Language Model)**: OpenAI와 같은 특정 언어 모델을 사용하여 텍스트를 생성합니다.\n",
      "- **LLMChain**: 입력 프롬프트를 받아 언어 모델을 호출하고, 결과를 반환합니다.\n",
      "\n",
      "### 4. 다양한 기능\n",
      "\n",
      "LangChain은 단순한 언어 모델 호출 뿐만 아니라, 다양한 기능을 제공합니다.\n",
      "\n",
      "- **문서 검색**: 특정 데이터 소스에서 정보를 검색할 수 있습니다.\n",
      "- **질문 응답**: 질문에 대한 답변을 제공하는 기능을 구현할 수 있습니다.\n",
      "- **체인 조합**: 여러 개의 체인을 결합하여 복잡한 작업을 수행할 수 있습니다.\n",
      "\n",
      "### 5. 예제: 질문 응답\n",
      "\n",
      "```python\n",
      "from langchain.chains import QuestionAnsweringChain\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "llm = OpenAI(model=\"text-davinci-003\")\n",
      "qa_chain = QuestionAnsweringChain(llm=llm)\n",
      "\n",
      "# 질문 및 문서 제공\n",
      "question = \"파이썬의 주요 특징은 무엇인가요?\"\n",
      "context = \"파이썬은 간결하고 명확한 문법을 가지고 있으며, 다양한 라이브러리를 지원합니다.\"\n",
      "\n",
      "# 질문에 대한 답변 생성\n",
      "answer = qa_chain.run(question, context)\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "### 6. 문서화 및 추가 자료\n",
      "\n",
      "LangChain은 활발히 개발되고 있으며, [공식 문서](https://langchain.readthedocs.io/en/latest/)를 참고하면 더 많은 기능과 사용 예제를 확인할 수 있습니다.\n",
      "\n",
      "이와 같은 방식으로 LangChain을 사용하여 다양한 자연어 처리 작업을 수행할 수 있습니다. 필요한 기능에 따라 코드를 수정하고 확장해보세요!\n",
      "총 토큰 사용량: 748\n",
      "총 프롬프트 토큰 사용량: 37\n",
      "총 응답 토큰 사용량: 711\n",
      "총 비용: 0.00043214999999999996\n"
     ]
    }
   ],
   "source": [
    "# 누적 토큰 사용량 확인 -> 콜백함수 이용( get_openai_callback )\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "with get_openai_callback() as cb:\n",
    "    # 첫 번째 호출\n",
    "    res1 = llm.invoke(\"오늘 서울의 날씨를 알려줘.\")\n",
    "    print(\"응답1:\", res1.content)\n",
    "    # 두 번째 호출\n",
    "    res2 = llm.invoke(\"파이썬으로 LangChain을 사용하는 방법을 알려줘.\")\n",
    "    print(\"응답2:\", res2.content)\n",
    "\n",
    "# 누적 토큰 사용량 출력\n",
    "# 콜백 cb에는 블록 내 전체 토큰 사용량이 누적\n",
    "# 총 토큰 사용량\n",
    "print(\"총 토큰 사용량:\", cb.total_tokens)\n",
    "# 총 프롬프트 토큰 사용량\n",
    "print(\"총 프롬프트 토큰 사용량:\", cb.prompt_tokens)\n",
    "# 총 응답 토큰 사용량\n",
    "print(\"총 응답 토큰 사용량:\", cb.completion_tokens)\n",
    "# 총 비용\n",
    "print(\"총 비용:\", cb.total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a513bf16",
   "metadata": {},
   "source": [
    "# LangChain의 LLM 응답 캐싱( In-Memory Caching, SQLiteCache )\n",
    "- 동일한 질문은 저장해 뒀다가 응답에 재사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d2cb1",
   "metadata": {},
   "source": [
    "## In-Memory Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9174c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.caches import InMemoryCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "# In-Memory Cache 설정\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f91fdb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응답1: 물론이죠! 다음은 간단한 유머입니다:\n",
      "\n",
      "왜 컴퓨터는 바다에 빠지지 않을까요?\n",
      "\n",
      "왜냐하면 \"다이브(Dive)\" 버튼이 없거든요! \n",
      "\n",
      "재미있으셨나요? 더 필요하시면 말씀해 주세요!\n",
      "응답2: 물론이죠! 다음은 간단한 유머입니다:\n",
      "\n",
      "왜 컴퓨터는 바다에 빠지지 않을까요?\n",
      "\n",
      "왜냐하면 \"다이브(Dive)\" 버튼이 없거든요! \n",
      "\n",
      "재미있으셨나요? 더 필요하시면 말씀해 주세요!\n"
     ]
    }
   ],
   "source": [
    "# 캐시 사용 전 후 비교\n",
    "# 같은 질문을 두 번 호출\n",
    "query = \"재미있는 유머 하나 알려줘\"\n",
    "# 첫 번째 호출: 캐시에 응답이 없으면 API 호출 발생\n",
    "response1 = llm.invoke(query)\n",
    "print(\"응답1:\", response1.content)\n",
    "# 두 번째 호출: 캐시에 응답이 있으면 API 호출 없이 캐시된 응답 사용\n",
    "response2 = llm.invoke(query)\n",
    "print(\"응답2:\", response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffc47e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 호출 시간: 6.4144초\n",
      "두 번째 호출 시간: 0.0007초\n"
     ]
    }
   ],
   "source": [
    "# 실행시간 측정\n",
    "import time\n",
    "\n",
    "query = \"오늘 점심 메뉴 추천해줘\"\n",
    "# 첫 번째 호출 시간\n",
    "start = time.time()\n",
    "llm.invoke(query)\n",
    "end = time.time()\n",
    "print(f\"첫 번째 호출 시간: {end - start:.4f}초\")\n",
    "# 두 번째 호출 시간 \n",
    "start = time.time()\n",
    "llm.invoke(query)\n",
    "end = time.time()\n",
    "print(f\"두 번째 호출 시간: {end - start:.4f}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c27e06",
   "metadata": {},
   "source": [
    "# SQLite 캐시( 디스크 기반 캐시 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb215b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응답1: 넌센스 퀴즈 하나 드릴게요!\n",
      "\n",
      "**문제:** \n",
      "어떤 행성이 가장 많은 친구를 가지고 있을까요?\n",
      "\n",
      "**정답:** \n",
      "화성! (화성은 '화'가 많아서 친구가 많다는 말장난이에요.) \n",
      "\n",
      "재미있죠? 또 궁금한 게 있으면 말씀해 주세요!\n",
      "첫 번째 호출 시간: 0.0033초\n",
      "----------------------------------------------------------------------------------------------------\n",
      "응답2: 넌센스 퀴즈 하나 드릴게요!\n",
      "\n",
      "**문제:** \n",
      "어떤 행성이 가장 많은 친구를 가지고 있을까요?\n",
      "\n",
      "**정답:** \n",
      "화성! (화성은 '화'가 많아서 친구가 많다는 말장난이에요.) \n",
      "\n",
      "재미있죠? 또 궁금한 게 있으면 말씀해 주세요!\n",
      "두 번째 호출 시간: 0.0011초\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "from langchain_community.cache import SQLiteCache\n",
    "\n",
    "# SQLite Cache 설정( 지정한 경로의 DB 파일을 생성 / 사용 )\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\")) # langchain.db 파일 생성\n",
    "\n",
    "# 동일한 query를 두 번 호출해서 결과와 시간을 비교\n",
    "query = \"넌센스 퀴즈 하나와 정답을 알려줘.\"\n",
    "# 첫 번째 호출: 캐시에 응답이 없으면 API 호출 발생\n",
    "start = time.time()\n",
    "response1 = llm.invoke(query)\n",
    "end = time.time()\n",
    "print(\"응답1:\", response1.content)\n",
    "print(f\"첫 번째 호출 시간: {end - start:.4f}초\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# 두 번째 호출: 캐시에 응답이 있으면 API 호출 없이 캐시된 응답 사용\n",
    "start = time.time()\n",
    "response2 = llm.invoke(query)\n",
    "end = time.time()\n",
    "print(\"응답2:\", response2.content)\n",
    "print(f\"두 번째 호출 시간: {end - start:.4f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5075e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
